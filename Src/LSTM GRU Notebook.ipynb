{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code process for demo of  LSTM /GRU  Network & Toturial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Toturial : [Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。\n",
    "\n",
    "但是这种普通的神经网络对于很多问题却无能无力。\n",
    "\n",
    "例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。\n",
    "\n",
    "RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。\n",
    "\n",
    "具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。\n",
    "\n",
    "理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先从RNN讲起\n",
    "Applies a multi-layer Elman RNN with tanhtanh or ReLUReLU non-linearity to an input sequence.\n",
    "For each element in the input sequence, each layer computes the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(10, 20, num_layers=2)\n",
      "tensor([[[-1.1454e-01, -1.7025e-01,  5.5742e-02, -4.3346e-01,  5.5446e-02,\n",
      "          -2.5968e-01, -3.8892e-01, -3.1633e-01, -1.5376e-02, -8.1861e-01,\n",
      "           3.9429e-01, -2.3296e-01,  5.1540e-01,  5.7671e-01,  3.7141e-01,\n",
      "          -5.1620e-01, -4.1394e-01, -1.6207e-01,  7.2390e-01, -8.0290e-01],\n",
      "         [ 2.4392e-01,  2.1431e-01, -3.1024e-01, -8.0635e-01,  7.9469e-01,\n",
      "          -5.7902e-01, -1.3277e-01,  2.7272e-02, -1.5678e-01,  2.3512e-01,\n",
      "          -8.0341e-03, -2.9435e-01, -6.9532e-01,  3.1768e-01,  8.5113e-01,\n",
      "           1.1736e-01, -4.9585e-01,  6.2332e-01,  5.4864e-01, -7.4009e-02],\n",
      "         [ 3.4614e-01, -2.4289e-02,  2.4763e-01,  3.3382e-01,  1.0847e-01,\n",
      "           5.5826e-01, -3.2925e-01, -4.2714e-01, -4.1176e-01, -1.6511e-01,\n",
      "          -8.0263e-01, -3.2738e-02,  4.1803e-02, -2.2655e-01, -5.3725e-01,\n",
      "          -1.6130e-01,  1.1606e-01, -4.3286e-01,  3.8388e-01,  1.9152e-01]],\n",
      "\n",
      "        [[ 7.3618e-02,  6.9184e-02,  4.1965e-04,  2.6711e-01, -1.9845e-02,\n",
      "          -2.2026e-01,  3.5632e-01, -8.7246e-02,  3.3657e-01, -1.3095e-01,\n",
      "           3.4691e-01, -3.3727e-02, -1.4778e-01, -2.8349e-01, -5.3851e-01,\n",
      "          -3.1510e-01, -6.2625e-02, -7.3419e-02, -2.8963e-01, -1.3326e-01],\n",
      "         [ 1.4597e-01,  1.8526e-01,  1.6193e-01,  1.4009e-02,  1.8453e-01,\n",
      "           1.9963e-01,  5.0607e-01,  5.2548e-01,  1.0782e-01,  2.9590e-01,\n",
      "           3.5369e-01, -1.5245e-01, -7.9276e-02,  1.7514e-01, -5.4937e-01,\n",
      "          -5.1949e-01, -5.8714e-01,  5.1101e-01, -1.4706e-01, -1.9451e-01],\n",
      "         [ 3.7111e-01,  1.2938e-01,  7.4158e-02, -5.6316e-01,  1.0986e-01,\n",
      "          -1.5701e-01,  6.3617e-02, -1.5672e-01, -2.4058e-01,  4.1228e-02,\n",
      "           4.6165e-01, -3.6742e-01,  6.8771e-02, -5.2084e-02, -8.7574e-02,\n",
      "           1.2928e-01,  3.9894e-01,  9.7533e-02, -1.7914e-03, -1.6017e-01]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "rnn=nn.RNN(10,20,2)\n",
    "print(rnn)\n",
    "input=torch.randn(5,3,10)\n",
    "h0=torch.randn(2,3,20)\n",
    "output,hn=rnn(input,h0)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "RNN隐状态计算公式：\n",
    "\n",
    "$$ \n",
    "H_t=tanh(w_{ih}*x_{t}+b_{ih}+w_{hh}*h_{t-1}+b_{hh})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element in the input sequence, each layer computes the following\n",
    "    function:\n",
    "\n",
    "\n",
    "$$\n",
    "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "            h_t = o_t * \\tanh(c_t) \\\\\n",
    "$$\n",
    "\n",
    "    where :\n",
    "\n",
    "$h_t $ is the hidden state at time $t$, \n",
    "\n",
    "\n",
    "$c_t$ is the cell state at time $t$ \n",
    "\n",
    "$x_t$ is the input at time $t$\n",
    "\n",
    "$h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden\n",
    "state at time $0$, \n",
    "\n",
    "$i_t$, $f_t$, $g_t$,\n",
    "\n",
    "$o_t$ are the input, forget, cell, and output gates, respectively.\n",
    "\n",
    "$\\sigma$ is the sigmoid function, and $*$ is the Hadamard product.\n",
    "\n",
    "In a multilayer LSTM, the input $x^{(l)}_t$ of the $l$ -th layer\n",
    "($l >= 2$) is the hidden state $h^{(l-1)}_t$ of the previous layer multiplied by\n",
    "dropout $\\delta^{(l-1)}_t$ where each $\\delta^{(l-1)}_t$ is a Bernoulli random\n",
    "variable which is $0$ with probability :attr:$dropout$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
